# Phase 2: AI Hallucination Research - Types, Causes & Prevention

## 1. Understanding & Analysis

### Core Issue

研究 AI 幻覺（Hallucination）的本質、分類、成因，以及業界最佳防範實踐，為 Cortex Protocol 設計系統化的防範機制。

### Context

-   **AI 幻覺定義**: AI 系統產生看似合理但實際上不準確、不存在或無法驗證的內容
-   **目標受眾**: 軟體開發場景中的 AI 助手（特別是代碼生成、分析、規劃）
-   **風險等級**: 高 - 幻覺可能導致錯誤的代碼、錯誤的架構決策、浪費的開發時間

### Chosen Approach

採用多維度分析框架：

1. **類型學分析**: 按內容類型分類幻覺
2. **成因分析**: 從 LLM 機制角度理解為何產生幻覺
3. **情境分析**: 在軟體開發場景中，哪些情況最容易產生幻覺
4. **防範策略**: 從 prompt engineering、架構設計、驗證機制三個層面設計防範措施

---

## 2. Deep Dive

### 2.1 AI 幻覺的類型學分類

基於軟體開發 AI 助手的具體應用場景，將幻覺分為以下六大類：

#### 2.1.1 Type 1: 事實性幻覺 (Factual Hallucination)

**風險評級**: ⚠️ **Critical** **定義**: AI 聲稱某個事實存在，但實際上不存在或不正確。 **示例**:

-   聲稱某個 API 函數存在但實際不存在
-   編造不存在的配置選項
-   聲稱某個依賴庫有某個功能但實際沒有

#### 2.1.2 Type 2: 上下文幻覺 (Contextual Hallucination)

**風險評級**: ⚠️ **High** **定義**: AI 對當前項目的具體上下文做出未經驗證的假設或推測。 **示例**:

-   假設項目中存在某個文件但實際未讀取
-   假設使用某個框架但未確認
-   對代碼依賴關係的錯誤推測

#### 2.1.3 Type 3: 能力幻覺 (Capability Hallucination)

**風險評級**: ⚠️ **Critical** **定義**: AI 宣稱已經執行或能夠執行某個操作，但實際上沒有或不能。 **示例**:

-   聲稱"已經修改了文件"但實際沒有成功
-   聲稱"已經運行了測試"但實際沒有執行
-   聲稱能夠"實時監控"但實際能力有限

#### 2.1.4 Type 4: 知識幻覺 (Knowledge Hallucination)

**風險評級**: ⚠️ **High** **定義**: AI 對不熟悉的技術表現出過度自信，提供看似專業但實際錯誤的建議。 **示例**:

-   對罕見技術棧給出錯誤的"最佳實踐"
-   混淆相似但不同的概念
-   對新技術的錯誤理解

#### 2.1.5 Type 5: 一致性幻覺 (Consistency Hallucination)

**風險評級**: ⚠️ **Medium** **定義**: AI 在對話中前後矛盾，或與先前提供的信息不一致。 **示例**:

-   先說"這是異步的"，後來又說"這是同步操作"
-   忘記對話早期建立的上下文

#### 2.1.6 Type 6: 引用幻覺 (Citation Hallucination)

**風險評級**: ⚠️ **Medium** **定義**: AI 編造文檔鏈接、issue 編號、commit hash 或其他引用信息。 **示例**:

-   提供不存在的文檔 URL
-   編造 GitHub issue 編號
-   引用不存在的規範

---

### 2.2 幻覺產生的深層機制

#### 2.2.1 LLM 固有特性

1. **概率性生成**: 基於 next-token prediction，選擇"最可能"而非"最正確"
2. **訓練數據局限**: Knowledge cutoff、數據質量問題、數據偏差
3. **缺乏真實接地**: 無法實時驗證、缺乏"我不知道"機制

#### 2.2.2 Prompt 設計問題

1. **角色扮演過度承諾**: "專家"角色導致過度自信
2. **用戶期望壓力**: 傾向於給出答案而非承認不知道
3. **上下文窗口限制**: 依賴推測填補信息缺口

---

### 2.3 高風險情境識別

**最容易產生幻覺的場景**:

1. 處理罕見技術棧或新技術
2. 跨文件推理時
3. 執行外部工具後報告結果
4. 長時間對話後期
5. 面對模糊或不完整的需求

---

### 2.4 業界最佳防範策略

#### 2.4.1 策略層面 1: Prompt Engineering

**Strategy 1: 明確的不確定性表達要求**

```
你必須區分以下三種陳述：
- [VERIFIED]: 我已通過工具驗證的事實
- [INFERRED]: 基於上下文的合理推測
- [UNCERTAIN]: 我不確定的信息

禁止在不確定時假裝確定。
```

**Strategy 2: 強制驗證流程**

```
在聲稱任何事實前，你必須：
1. 明確說明信息來源
2. 如果未驗證，必須使用工具驗證
3. 如果無法驗證，明確說明"未驗證"
```

**Strategy 3: 信心水平量化**

```
對每個建議或聲明，附上信心水平：
- HIGH (90-100%): 已驗證或來自確定知識
- MEDIUM (60-90%): 基於常見模式的推測
- LOW (<60%): 不確定或需要驗證
```

#### 2.4.2 策略層面 2: 架構設計

**Strategy 4: 證據鏈機制 (Chain of Evidence)**

-   所有聲明必須追溯到證據來源
-   工具調用結果作為主要證據
-   明確區分"已讀取的文件"vs"假設存在的文件"

**Strategy 5: 自我質疑循環 (Self-Questioning Loop)**

```
在輸出前自問：
1. 我聲稱的這個事實，我真的驗證過嗎？
2. 如果沒有，我是否明確標記為"推測"？
3. 用戶會不會誤以為我已經驗證？
```

**Strategy 6: 工具調用強制確認**

-   所有工具調用必須等待實際結果
-   禁止"預測"工具調用結果
-   失敗的工具調用必須明確報告

#### 2.4.3 策略層面 3: 驗證與質量門檻

**Strategy 7: 輸出前自檢清單**

```
在完成任務前檢查：
□ 所有事實聲明都有證據支持或明確標記為推測
□ 所有工具調用都成功執行並確認結果
□ 沒有聲稱執行但未實際執行的操作
□ 對不確定的內容使用了恰當的限定語
```

**Strategy 8: 人類審查觸發點**

-   當信心水平 < 70% 時，主動請求人類確認
-   當無法驗證關鍵假設時，明確警告
-   當操作涉及高風險時（如刪除代碼），強制確認

**Strategy 9: 對話歷史交叉驗證**

-   在長對話中，定期回顧關鍵決定
-   檢測前後矛盾
-   必要時主動向用戶確認先前的上下文

---

### 2.5 特定於軟體開發的防範規則

#### 2.5.1 代碼生成場景

**Rule 1: API 使用驗證**

-   在使用任何 API 前，必須先讀取文檔或查看實際定義
-   對於不確定的 API，使用 "// TODO: Verify API signature" 註釋

**Rule 2: 文件存在性檢查**

-   在聲稱文件存在前，使用 `list_files` 或 `read_file` 驗證
-   使用明確語言: "我檢查了 X 目錄，發現..."

**Rule 3: 依賴關係明確性**

-   在建議安裝依賴時，說明版本兼容性
-   對於不確定的版本，建議"請確認..."

#### 2.5.2 調試場景

**Rule 4: 錯誤診斷證據鏈**

-   必須提供完整錯誤輸出（Evidentiary Protocol）
-
